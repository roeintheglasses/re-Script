version: '3.8'

services:
  # Redis - Message queue and caching
  redis:
    image: redis:7-alpine
    container_name: re-script-redis
    restart: unless-stopped
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes --maxmemory 256mb --maxmemory-policy allkeys-lru
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 3
    networks:
      - re-script-network

  # Web API - Backend service
  web-api:
    build:
      context: .
      dockerfile: apps/web-api/Dockerfile
    container_name: re-script-api
    restart: unless-stopped
    ports:
      - "3001:3001"
    environment:
      NODE_ENV: production
      HOST: "0.0.0.0"
      PORT: 3001
      REDIS_URL: redis://redis:6379
      CORS_ORIGIN: http://localhost:3000
      # Add your LLM provider API keys here
      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY:-}
      OPENAI_API_KEY: ${OPENAI_API_KEY:-}
      OLLAMA_BASE_URL: ${OLLAMA_BASE_URL:-http://ollama:11434}
    depends_on:
      redis:
        condition: service_healthy
    volumes:
      - ./uploads:/app/uploads
      - ./logs:/app/logs
    networks:
      - re-script-network
    healthcheck:
      test: ["CMD", "node", "-e", "require('http').get('http://localhost:3001/health', (r) => process.exit(r.statusCode === 200 ? 0 : 1))"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Web UI - Frontend service
  web-ui:
    build:
      context: .
      dockerfile: apps/web-ui/Dockerfile
    container_name: re-script-web
    restart: unless-stopped
    ports:
      - "3000:3000"
    environment:
      NODE_ENV: production
      HOSTNAME: "0.0.0.0"
      PORT: 3000
      NEXT_PUBLIC_API_URL: http://localhost:3001
    depends_on:
      web-api:
        condition: service_healthy
    networks:
      - re-script-network
    healthcheck:
      test: ["CMD", "node", "-e", "require('http').get('http://localhost:3000/api/health', (r) => process.exit(r.statusCode === 200 ? 0 : 1))"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Optional: Ollama for local LLM processing
  ollama:
    image: ollama/ollama:latest
    container_name: re-script-ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      OLLAMA_HOST: "0.0.0.0"
    networks:
      - re-script-network
    profiles:
      - local-llm
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3

volumes:
  redis_data:
    driver: local
  ollama_data:
    driver: local

networks:
  re-script-network:
    driver: bridge